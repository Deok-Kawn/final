{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Task 5: LSTM Model Implementation\n",
        "\n",
        "## 🧠 한국 전력 수요 예측 - LSTM 딥러닝 모델\n",
        "\n",
        "**목표**: PyTorch를 사용한 LSTM 모델로 2024.1.1~2025.6.10 (527일) 예측\n",
        "\n",
        "**구현 내용:**\n",
        "1. **PyTorch LSTM 아키텍처**: 시계열 예측용 모델 설계\n",
        "2. **Custom Dataset & DataLoader**: 시계열 데이터 전용 파이프라인\n",
        "3. **훈련 루프**: Early stopping, 학습률 스케줄러 포함\n",
        "4. **527일 예측**: Multi-step forecasting 구현\n",
        "5. **성능 평가**: RMSE로 베이스라인 모델과 비교\n",
        "\n",
        "**데이터:**\n",
        "- 훈련: `results/preprocessing/05_data_splitting/train_data_full.csv` (6,939일 × 25열)\n",
        "- 예측 템플릿: `results/preprocessing/05_data_splitting/prediction_template.csv` (527일 × 19열)\n",
        "- CV 폴드: 시계열 교차검증 (4 folds)\n",
        "\n",
        "**하이퍼파라미터 실험:**\n",
        "- Sequence Length: 30, 60, 90일\n",
        "- Hidden Size: 64, 128, 256\n",
        "- Number of Layers: 1, 2, 3\n",
        "- Batch Size: 32, 64, 128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필수 패키지 설치 및 import\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# 핵심 패키지들 설치\n",
        "packages_to_install = {\n",
        "    'torch': 'torch',\n",
        "    'torchvision': 'torchvision', \n",
        "    'scikit-learn': 'sklearn',\n",
        "    'matplotlib': 'matplotlib',\n",
        "    'seaborn': 'seaborn',\n",
        "    'joblib': 'joblib'\n",
        "}\n",
        "\n",
        "print(\"🔄 필수 패키지 설치 확인 중...\")\n",
        "\n",
        "for pip_name, import_name in packages_to_install.items():\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "        print(f\"✅ {pip_name} 이미 설치됨\")\n",
        "    except ImportError:\n",
        "        print(f\"⚠️ {pip_name} 설치 중...\")\n",
        "        if pip_name == 'torch':\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"--user\"])\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name, \"--user\"])\n",
        "        print(f\"✅ {pip_name} 설치 완료\")\n",
        "\n",
        "# 필수 라이브러리 임포트\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import json\n",
        "    import joblib\n",
        "    from datetime import datetime, timedelta\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "    \n",
        "    # PyTorch 관련\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import Dataset, DataLoader, random_split\n",
        "    \n",
        "    print(\"✅ 모든 라이브러리 임포트 성공!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"❌ 라이브러리 임포트 실패: {e}\")\n",
        "    print(\"📋 재실행 후 다시 시도해주세요.\")\n",
        "    raise\n",
        "\n",
        "# 한글 폰트 설정 (시스템 폰트 자동 감지)\n",
        "try:\n",
        "    import matplotlib.font_manager as fm\n",
        "    \n",
        "    # macOS 시스템 한글 폰트 찾기\n",
        "    system_fonts = [f.name for f in fm.fontManager.ttflist]\n",
        "    korean_fonts = [f for f in system_fonts if any(keyword in f for keyword in ['Gothic', 'Malgun', 'NanumGothic', 'AppleGothic'])]\n",
        "    \n",
        "    if korean_fonts:\n",
        "        plt.rcParams['font.family'] = korean_fonts[0]\n",
        "        print(f\"✅ 한글 폰트 설정: {korean_fonts[0]}\")\n",
        "    else:\n",
        "        plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "        print(\"⚠️ 한글 폰트를 찾을 수 없어 기본 폰트 사용\")\n",
        "        \n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️ 폰트 설정 오류 (무시하고 진행): {e}\")\n",
        "    plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# GPU/CPU 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🔧 디바이스: {device}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📦 라이브러리 로드 완료!\")\n",
        "print(f\"📅 현재 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"🐍 Python 버전: {sys.version.split()[0]}\")\n",
        "print(f\"🔥 PyTorch 버전: {torch.__version__}\")\n",
        "print(f\"📚 주요 패키지 버전:\")\n",
        "print(f\"   - pandas: {pd.__version__}\")\n",
        "print(f\"   - numpy: {np.__version__}\")\n",
        "print(f\"   - sklearn: {sklearn.__version__}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 로드 및 전처리\n",
        "print(\"🔄 데이터 로드 중...\")\n",
        "\n",
        "# 훈련 데이터 로드 (Task 3.5 결과)\n",
        "train_path = '../../results/preprocessing/05_data_splitting/train_data_full.csv'\n",
        "train_data = pd.read_csv(train_path)\n",
        "train_data['date'] = pd.to_datetime(train_data['date'])\n",
        "\n",
        "# 예측 템플릿 로드\n",
        "pred_template_path = '../../results/preprocessing/05_data_splitting/prediction_template.csv'\n",
        "pred_template = pd.read_csv(pred_template_path)\n",
        "pred_template['date'] = pd.to_datetime(pred_template['date'])\n",
        "\n",
        "# Lag 초기값 로드\n",
        "lag_init_path = '../../results/preprocessing/05_data_splitting/lag_initialization.json'\n",
        "with open(lag_init_path, 'r') as f:\n",
        "    lag_init = json.load(f)\n",
        "\n",
        "# CV 폴드 정보 로드\n",
        "cv_metadata_path = '../../results/preprocessing/05_data_splitting/cv_folds_metadata.json'\n",
        "with open(cv_metadata_path, 'r') as f:\n",
        "    cv_metadata = json.load(f)\n",
        "\n",
        "print(f\"✅ 훈련 데이터: {train_data.shape} (기간: {train_data['date'].min()} ~ {train_data['date'].max()})\")\n",
        "print(f\"✅ 예측 템플릿: {pred_template.shape} (기간: {pred_template['date'].min()} ~ {pred_template['date'].max()})\")\n",
        "print(f\"✅ CV 설정: {cv_metadata['metadata']['total_folds']}개 폴드\")\n",
        "\n",
        "# LSTM 입력을 위한 피처 선택\n",
        "feature_columns = [col for col in train_data.columns if col not in ['date', '최대전력(MW)']]\n",
        "target_column = '최대전력(MW)'\n",
        "\n",
        "print(f\"✅ 선택된 피처 수: {len(feature_columns)}\")\n",
        "print(f\"✅ 타겟 변수: {target_column}\")\n",
        "\n",
        "# 데이터 정렬 (날짜순)\n",
        "train_data = train_data.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n📊 타겟 변수 통계:\")\n",
        "print(f\"   평균: {train_data[target_column].mean():.0f} MW\")\n",
        "print(f\"   표준편차: {train_data[target_column].std():.0f} MW\")\n",
        "print(f\"   범위: {train_data[target_column].min():.0f} ~ {train_data[target_column].max():.0f} MW\")\n",
        "\n",
        "# 데이터 전처리를 위한 스케일러 설정\n",
        "feature_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "\n",
        "# 훈련 데이터로 스케일러 학습\n",
        "features_scaled = feature_scaler.fit_transform(train_data[feature_columns])\n",
        "target_scaled = target_scaler.fit_transform(train_data[[target_column]])\n",
        "\n",
        "print(\"✅ 데이터 스케일링 완료 (MinMaxScaler)\")\n",
        "print(f\"   피처 스케일 범위: [0, 1]\")\n",
        "print(f\"   타겟 스케일 범위: [0, 1]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Dataset 클래스 정의\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"시계열 데이터를 위한 Custom Dataset\"\"\"\n",
        "    \n",
        "    def __init__(self, features, targets, sequence_length=60):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: 스케일된 피처 데이터 (numpy array)\n",
        "            targets: 스케일된 타겟 데이터 (numpy array)\n",
        "            sequence_length: 입력 시퀀스 길이\n",
        "        \"\"\"\n",
        "        self.features = features\n",
        "        self.targets = targets.flatten()  # (n, 1) -> (n,)\n",
        "        self.sequence_length = sequence_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.features) - self.sequence_length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # idx부터 sequence_length만큼의 피처 시퀀스\n",
        "        feature_seq = self.features[idx:idx+self.sequence_length]\n",
        "        \n",
        "        # idx+sequence_length 시점의 타겟값\n",
        "        target = self.targets[idx+self.sequence_length]\n",
        "        \n",
        "        return torch.FloatTensor(feature_seq), torch.FloatTensor([target])\n",
        "\n",
        "# LSTM 모델 클래스 정의\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"전력 수요 예측을 위한 LSTM 모델\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size: 입력 피처 수\n",
        "            hidden_size: LSTM 은닉층 크기\n",
        "            num_layers: LSTM 레이어 수\n",
        "            dropout: 드롭아웃 비율\n",
        "        \"\"\"\n",
        "        super(LSTMModel, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # LSTM 레이어\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        # 출력 레이어\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # LSTM 출력\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        \n",
        "        # 마지막 시점의 출력 사용\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        \n",
        "        # 드롭아웃 적용\n",
        "        dropped = self.dropout(last_output)\n",
        "        \n",
        "        # 선형 변환으로 최종 출력\n",
        "        output = self.fc(dropped)\n",
        "        \n",
        "        return output\n",
        "\n",
        "# 모델 하이퍼파라미터 설정\n",
        "SEQUENCE_LENGTH = 60  # 60일 시퀀스로 다음날 예측\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 100\n",
        "PATIENCE = 10  # Early stopping patience\n",
        "\n",
        "print(\"🏗️ 모델 아키텍처 정의 완료!\")\n",
        "print(f\"📐 하이퍼파라미터:\")\n",
        "print(f\"   - Sequence Length: {SEQUENCE_LENGTH}\")\n",
        "print(f\"   - Hidden Size: {HIDDEN_SIZE}\")\n",
        "print(f\"   - Num Layers: {NUM_LAYERS}\")\n",
        "print(f\"   - Dropout: {DROPOUT}\")\n",
        "print(f\"   - Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"   - Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"   - Max Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   - Early Stopping Patience: {PATIENCE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 훈련 및 검증 함수 정의\n",
        "def train_model(model, train_loader, val_loader, num_epochs, patience=10):\n",
        "    \"\"\"LSTM 모델 훈련\"\"\"\n",
        "    \n",
        "    # 손실 함수와 옵티마이저\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "    \n",
        "    # 훈련 기록\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "    \n",
        "    print(f\"🚀 모델 훈련 시작 (디바이스: {device})\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # 훈련 모드\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        \n",
        "        for batch_features, batch_targets in train_loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            batch_targets = batch_targets.to(device)\n",
        "            \n",
        "            # 순전파\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_targets)\n",
        "            \n",
        "            # 역전파\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # 검증 모드\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_features, batch_targets in val_loader:\n",
        "                batch_features = batch_features.to(device)\n",
        "                batch_targets = batch_targets.to(device)\n",
        "                \n",
        "                outputs = model(batch_features)\n",
        "                loss = criterion(outputs, batch_targets)\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        # 평균 손실 계산\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        # 학습률 스케줄러 업데이트\n",
        "        scheduler.step(avg_val_loss)\n",
        "        \n",
        "        # Early stopping 체크\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        # 진행상황 출력 (10 epoch마다)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1:3d}/{num_epochs}] | \"\n",
        "                  f\"Train Loss: {avg_train_loss:.6f} | \"\n",
        "                  f\"Val Loss: {avg_val_loss:.6f} | \"\n",
        "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n⏹️ Early stopping at epoch {epoch+1}\")\n",
        "            print(f\"   Best validation loss: {best_val_loss:.6f}\")\n",
        "            break\n",
        "    \n",
        "    # 최고 모델 복원\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"✅ 최고 성능 모델 복원 (Val Loss: {best_val_loss:.6f})\")\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "\n",
        "def calculate_rmse(y_true, y_pred):\n",
        "    \"\"\"RMSE 계산\"\"\"\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print(\"🔧 훈련 함수 정의 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 시계열 교차검증으로 모델 훈련\n",
        "print(\"🔄 시계열 교차검증 시작...\")\n",
        "\n",
        "# 결과 저장용\n",
        "cv_results = []\n",
        "models_dict = {}\n",
        "\n",
        "# 각 폴드별 훈련\n",
        "for fold_id, fold_info in cv_metadata['folds'].items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🔄 {fold_id} 훈련 시작\")\n",
        "    print(f\"   훈련: {fold_info['train_size']}일, 검증: {fold_info['val_size']}일\")\n",
        "    print(f\"   훈련 기간: {fold_info['train_start']} ~ {fold_info['train_end']}\")\n",
        "    print(f\"   검증 기간: {fold_info['val_start']} ~ {fold_info['val_end']}\")\n",
        "    \n",
        "    # 폴드별 데이터 분할\n",
        "    train_start_idx = 0\n",
        "    train_end_idx = fold_info['train_size']\n",
        "    val_start_idx = train_end_idx\n",
        "    val_end_idx = val_start_idx + fold_info['val_size']\n",
        "    \n",
        "    # 훈련/검증 데이터 준비\n",
        "    train_features = features_scaled[:train_end_idx]\n",
        "    train_targets = target_scaled[:train_end_idx]\n",
        "    val_features = features_scaled[val_start_idx:val_end_idx]\n",
        "    val_targets = target_scaled[val_start_idx:val_end_idx]\n",
        "    \n",
        "    # Dataset 및 DataLoader 생성\n",
        "    train_dataset = TimeSeriesDataset(train_features, train_targets, SEQUENCE_LENGTH)\n",
        "    val_dataset = TimeSeriesDataset(val_features, val_targets, SEQUENCE_LENGTH)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "    print(f\"   데이터셋 크기: 훈련 {len(train_dataset)}, 검증 {len(val_dataset)}\")\n",
        "    \n",
        "    # 모델 초기화\n",
        "    input_size = len(feature_columns)\n",
        "    model = LSTMModel(\n",
        "        input_size=input_size,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    \n",
        "    print(f\"   모델 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    # 모델 훈련\n",
        "    train_losses, val_losses = train_model(\n",
        "        model, train_loader, val_loader, NUM_EPOCHS, PATIENCE\n",
        "    )\n",
        "    \n",
        "    # 검증 데이터로 RMSE 계산\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_actuals = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_targets in val_loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            outputs = model(batch_features)\n",
        "            \n",
        "            # CPU로 이동 및 스케일 복원\n",
        "            pred_scaled = outputs.cpu().numpy()\n",
        "            target_scaled_batch = batch_targets.cpu().numpy()\n",
        "            \n",
        "            pred_original = target_scaler.inverse_transform(pred_scaled)\n",
        "            target_original = target_scaler.inverse_transform(target_scaled_batch)\n",
        "            \n",
        "            val_predictions.extend(pred_original.flatten())\n",
        "            val_actuals.extend(target_original.flatten())\n",
        "    \n",
        "    # RMSE 계산\n",
        "    fold_rmse = calculate_rmse(val_actuals, val_predictions)\n",
        "    \n",
        "    # 결과 저장\n",
        "    fold_result = {\n",
        "        'fold': fold_id,\n",
        "        'train_size': fold_info['train_size'],\n",
        "        'val_size': fold_info['val_size'],\n",
        "        'val_rmse': fold_rmse,\n",
        "        'final_train_loss': train_losses[-1],\n",
        "        'final_val_loss': val_losses[-1],\n",
        "        'epochs_trained': len(train_losses)\n",
        "    }\n",
        "    cv_results.append(fold_result)\n",
        "    models_dict[fold_id] = model.state_dict().copy()\n",
        "    \n",
        "    print(f\"✅ {fold_id} 완료!\")\n",
        "    print(f\"   검증 RMSE: {fold_rmse:.2f} MW\")\n",
        "    print(f\"   훈련 완료 에포크: {len(train_losses)}\")\n",
        "\n",
        "# 교차검증 결과 요약\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"📊 교차검증 결과 요약\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "cv_rmses = [result['val_rmse'] for result in cv_results]\n",
        "print(f\"평균 RMSE: {np.mean(cv_rmses):.2f} ± {np.std(cv_rmses):.2f} MW\")\n",
        "print(f\"최고 RMSE: {min(cv_rmses):.2f} MW\")\n",
        "print(f\"최악 RMSE: {max(cv_rmses):.2f} MW\")\n",
        "\n",
        "print(f\"\\n폴드별 상세 결과:\")\n",
        "for result in cv_results:\n",
        "    print(f\"  {result['fold']}: {result['val_rmse']:.2f} MW \"\n",
        "          f\"({result['epochs_trained']} epochs)\")\n",
        "\n",
        "print(\"✅ 교차검증 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 최종 모델 훈련 (전체 데이터)\n",
        "print(\"🎯 최종 모델 훈련 (전체 훈련 데이터 사용)\")\n",
        "\n",
        "# 전체 훈련 데이터로 Dataset 생성\n",
        "full_dataset = TimeSeriesDataset(features_scaled, target_scaled, SEQUENCE_LENGTH)\n",
        "\n",
        "# 80-20 분할 (훈련-검증)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"전체 데이터셋: {len(full_dataset)} samples\")\n",
        "print(f\"훈련: {len(train_dataset)}, 검증: {len(val_dataset)}\")\n",
        "\n",
        "# 최종 모델 초기화\n",
        "final_model = LSTMModel(\n",
        "    input_size=len(feature_columns),\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "print(f\"최종 모델 파라미터 수: {sum(p.numel() for p in final_model.parameters()):,}\")\n",
        "\n",
        "# 최종 모델 훈련\n",
        "print(\"\\n🚀 최종 모델 훈련 시작...\")\n",
        "final_train_losses, final_val_losses = train_model(\n",
        "    final_model, train_loader, val_loader, NUM_EPOCHS, PATIENCE\n",
        ")\n",
        "\n",
        "print(\"✅ 최종 모델 훈련 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 527일 예측 생성\n",
        "print(\"🔮 527일 예측 생성 중...\")\n",
        "\n",
        "def generate_future_predictions(model, last_sequence, pred_template, feature_scaler, target_scaler, sequence_length):\n",
        "    \"\"\"527일 미래 예측 생성\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    \n",
        "    # 초기 시퀀스 설정 (마지막 sequence_length일의 피처)\n",
        "    current_sequence = last_sequence.copy()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for day_idx in range(len(pred_template)):\n",
        "            # 현재 시퀀스를 텐서로 변환\n",
        "            seq_tensor = torch.FloatTensor(current_sequence).unsqueeze(0).to(device)\n",
        "            \n",
        "            # 예측\n",
        "            pred_scaled = model(seq_tensor).cpu().numpy()\n",
        "            pred_original = target_scaler.inverse_transform(pred_scaled)[0, 0]\n",
        "            predictions.append(pred_original)\n",
        "            \n",
        "            # 다음 날의 피처 가져오기 (예측 템플릿에서)\n",
        "            if day_idx < len(pred_template) - 1:\n",
        "                next_day_features = pred_template.iloc[day_idx][feature_columns].values\n",
        "                next_day_features_scaled = feature_scaler.transform([next_day_features])[0]\n",
        "                \n",
        "                # 시퀀스 업데이트 (가장 오래된 것 제거, 새로운 것 추가)\n",
        "                current_sequence = np.vstack([current_sequence[1:], next_day_features_scaled])\n",
        "    \n",
        "    return np.array(predictions)\n",
        "\n",
        "# 마지막 sequence_length일의 피처 추출\n",
        "last_sequence = features_scaled[-SEQUENCE_LENGTH:]\n",
        "\n",
        "# 527일 예측 생성\n",
        "future_predictions = generate_future_predictions(\n",
        "    final_model, last_sequence, pred_template, \n",
        "    feature_scaler, target_scaler, SEQUENCE_LENGTH\n",
        ")\n",
        "\n",
        "print(f\"✅ 527일 예측 완료!\")\n",
        "print(f\"   예측 범위: {future_predictions.min():.0f} ~ {future_predictions.max():.0f} MW\")\n",
        "print(f\"   예측 평균: {future_predictions.mean():.0f} MW\")\n",
        "\n",
        "# 대회 형식 날짜 변환 함수\n",
        "def format_competition_date(date):\n",
        "    \"\"\"YYYY.M.D 형식으로 날짜 변환 (한 자리 숫자일 때 0 생략)\"\"\"\n",
        "    return f\"{date.year}.{date.month}.{date.day}\"\n",
        "\n",
        "# 제출 파일 생성\n",
        "submission = pd.DataFrame({\n",
        "    'date': [format_competition_date(date) for date in pred_template['date']],\n",
        "    '최대전력(MW)': future_predictions\n",
        "})\n",
        "\n",
        "# 제출 파일 저장\n",
        "submission_path = 'submission_lstm.csv'\n",
        "submission.to_csv(submission_path, index=False)\n",
        "print(f\"✅ 제출 파일 저장: {submission_path}\")\n",
        "\n",
        "# 제출 파일 미리보기\n",
        "print(f\"\\n📋 제출 파일 미리보기:\")\n",
        "print(submission.head(10))\n",
        "print(\"...\")\n",
        "print(submission.tail(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 결과 시각화 및 모델 저장\n",
        "print(\"📊 결과 시각화 및 모델 저장...\")\n",
        "\n",
        "# 1. 훈련 손실 시각화\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 첫 번째 서브플롯: 최종 모델 훈련 손실\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(final_train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(final_val_losses, label='Validation Loss', color='red')\n",
        "plt.title('Final Model Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 두 번째 서브플롯: CV RMSE 분포\n",
        "plt.subplot(2, 3, 2)\n",
        "cv_rmses = [result['val_rmse'] for result in cv_results]\n",
        "plt.bar(range(len(cv_rmses)), cv_rmses, color='skyblue', edgecolor='navy')\n",
        "plt.title('Cross-Validation RMSE by Fold')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('RMSE (MW)')\n",
        "plt.xticks(range(len(cv_rmses)), [f'Fold {i+1}' for i in range(len(cv_rmses))])\n",
        "for i, rmse in enumerate(cv_rmses):\n",
        "    plt.text(i, rmse + 50, f'{rmse:.0f}', ha='center', va='bottom')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 세 번째 서브플롯: 예측값 분포\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.hist(future_predictions, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "plt.title('Prediction Distribution')\n",
        "plt.xlabel('Power Demand (MW)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(future_predictions.mean(), color='red', linestyle='--', \n",
        "           label=f'Mean: {future_predictions.mean():.0f} MW')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 네 번째 서브플롯: 월별 예측 패턴\n",
        "plt.subplot(2, 3, 4)\n",
        "pred_dates = pred_template['date']\n",
        "monthly_avg = []\n",
        "months = []\n",
        "for month in range(1, 13):\n",
        "    month_mask = pred_dates.dt.month == month\n",
        "    if month_mask.any():\n",
        "        monthly_avg.append(future_predictions[month_mask].mean())\n",
        "        months.append(month)\n",
        "\n",
        "plt.plot(months, monthly_avg, marker='o', linewidth=2, markersize=6)\n",
        "plt.title('Monthly Average Predictions')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Power Demand (MW)')\n",
        "plt.xticks(months)\n",
        "plt.grid(True)\n",
        "\n",
        "# 다섯 번째 서브플롯: 시계열 예측 그래프 (첫 90일)\n",
        "plt.subplot(2, 3, 5)\n",
        "first_90_days = pred_dates[:90]\n",
        "first_90_predictions = future_predictions[:90]\n",
        "plt.plot(first_90_days, first_90_predictions, linewidth=1.5, color='purple')\n",
        "plt.title('First 90 Days Predictions')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Power Demand (MW)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "# 여섯 번째 서브플롯: 모델 성능 요약\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.axis('off')\n",
        "summary_text = f'''\n",
        "LSTM Model Performance Summary\n",
        "\n",
        "Cross-Validation Results:\n",
        "  Mean RMSE: {np.mean(cv_rmses):.1f} ± {np.std(cv_rmses):.1f} MW\n",
        "  Best RMSE: {min(cv_rmses):.1f} MW\n",
        "  Worst RMSE: {max(cv_rmses):.1f} MW\n",
        "\n",
        "Model Architecture:\n",
        "  Input Features: {len(feature_columns)}\n",
        "  Hidden Size: {HIDDEN_SIZE}\n",
        "  Num Layers: {NUM_LAYERS}\n",
        "  Sequence Length: {SEQUENCE_LENGTH}\n",
        "  Parameters: {sum(p.numel() for p in final_model.parameters()):,}\n",
        "\n",
        "Prediction Stats:\n",
        "  Mean: {future_predictions.mean():.0f} MW\n",
        "  Std: {future_predictions.std():.0f} MW\n",
        "  Range: {future_predictions.min():.0f} - {future_predictions.max():.0f} MW\n",
        "'''\n",
        "plt.text(0.05, 0.95, summary_text, fontsize=10, verticalalignment='top',\n",
        "         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('lstm_model_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 모델 및 스케일러 저장\n",
        "print(\"\\n💾 모델 및 메타데이터 저장...\")\n",
        "\n",
        "# 모델 저장\n",
        "torch.save({\n",
        "    'model_state_dict': final_model.state_dict(),\n",
        "    'model_architecture': {\n",
        "        'input_size': len(feature_columns),\n",
        "        'hidden_size': HIDDEN_SIZE,\n",
        "        'num_layers': NUM_LAYERS,\n",
        "        'dropout': DROPOUT\n",
        "    },\n",
        "    'training_config': {\n",
        "        'sequence_length': SEQUENCE_LENGTH,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'num_epochs': NUM_EPOCHS,\n",
        "        'patience': PATIENCE\n",
        "    },\n",
        "    'feature_columns': feature_columns,\n",
        "    'target_column': target_column\n",
        "}, 'lstm_model.pth')\n",
        "\n",
        "# 스케일러 저장\n",
        "joblib.dump(feature_scaler, 'feature_scaler_lstm.pkl')\n",
        "joblib.dump(target_scaler, 'target_scaler_lstm.pkl')\n",
        "\n",
        "# 결과 메타데이터 저장\n",
        "metadata = {\n",
        "    'model_type': 'LSTM',\n",
        "    'creation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'cv_results': cv_results,\n",
        "    'final_model_performance': {\n",
        "        'final_train_loss': final_train_losses[-1],\n",
        "        'final_val_loss': final_val_losses[-1],\n",
        "        'epochs_trained': len(final_train_losses)\n",
        "    },\n",
        "    'prediction_stats': {\n",
        "        'mean': float(future_predictions.mean()),\n",
        "        'std': float(future_predictions.std()),\n",
        "        'min': float(future_predictions.min()),\n",
        "        'max': float(future_predictions.max())\n",
        "    },\n",
        "    'hyperparameters': {\n",
        "        'sequence_length': SEQUENCE_LENGTH,\n",
        "        'hidden_size': HIDDEN_SIZE,\n",
        "        'num_layers': NUM_LAYERS,\n",
        "        'dropout': DROPOUT,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('lstm_model_metadata.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"✅ 저장 완료!\")\n",
        "print(\"   - lstm_model.pth: 모델 가중치\")\n",
        "print(\"   - feature_scaler_lstm.pkl: 피처 스케일러\")\n",
        "print(\"   - target_scaler_lstm.pkl: 타겟 스케일러\")\n",
        "print(\"   - lstm_model_metadata.json: 모델 메타데이터\")\n",
        "print(\"   - submission_lstm.csv: 대회 제출 파일\")\n",
        "print(\"   - lstm_model_analysis.png: 분석 시각화\")\n",
        "\n",
        "print(f\"\\n🎯 Task 5 완료!\")\n",
        "print(f\"   평균 CV RMSE: {np.mean(cv_rmses):.1f} MW\")\n",
        "print(f\"   제출 파일: submission_lstm.csv ({len(submission)} 예측)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
