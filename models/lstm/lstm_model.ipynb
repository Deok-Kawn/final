{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Task 5: LSTM Model Implementation\n",
        "\n",
        "## ğŸ§  í•œêµ­ ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ - LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸\n",
        "\n",
        "**ëª©í‘œ**: PyTorchë¥¼ ì‚¬ìš©í•œ LSTM ëª¨ë¸ë¡œ 2024.1.1~2025.6.10 (527ì¼) ì˜ˆì¸¡\n",
        "\n",
        "**êµ¬í˜„ ë‚´ìš©:**\n",
        "1. **PyTorch LSTM ì•„í‚¤í…ì²˜**: ì‹œê³„ì—´ ì˜ˆì¸¡ìš© ëª¨ë¸ ì„¤ê³„\n",
        "2. **Custom Dataset & DataLoader**: ì‹œê³„ì—´ ë°ì´í„° ì „ìš© íŒŒì´í”„ë¼ì¸\n",
        "3. **í›ˆë ¨ ë£¨í”„**: Early stopping, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ í¬í•¨\n",
        "4. **527ì¼ ì˜ˆì¸¡**: Multi-step forecasting êµ¬í˜„\n",
        "5. **ì„±ëŠ¥ í‰ê°€**: RMSEë¡œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ê³¼ ë¹„êµ\n",
        "\n",
        "**ë°ì´í„°:**\n",
        "- í›ˆë ¨: `results/preprocessing/05_data_splitting/train_data_full.csv` (6,939ì¼ Ã— 25ì—´)\n",
        "- ì˜ˆì¸¡ í…œí”Œë¦¿: `results/preprocessing/05_data_splitting/prediction_template.csv` (527ì¼ Ã— 19ì—´)\n",
        "- CV í´ë“œ: ì‹œê³„ì—´ êµì°¨ê²€ì¦ (4 folds)\n",
        "\n",
        "**í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜:**\n",
        "- Sequence Length: 30, 60, 90ì¼\n",
        "- Hidden Size: 64, 128, 256\n",
        "- Number of Layers: 1, 2, 3\n",
        "- Batch Size: 32, 64, 128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° import\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# í•µì‹¬ íŒ¨í‚¤ì§€ë“¤ ì„¤ì¹˜\n",
        "packages_to_install = {\n",
        "    'torch': 'torch',\n",
        "    'torchvision': 'torchvision', \n",
        "    'scikit-learn': 'sklearn',\n",
        "    'matplotlib': 'matplotlib',\n",
        "    'seaborn': 'seaborn',\n",
        "    'joblib': 'joblib'\n",
        "}\n",
        "\n",
        "print(\"ğŸ”„ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸ ì¤‘...\")\n",
        "\n",
        "for pip_name, import_name in packages_to_install.items():\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "        print(f\"âœ… {pip_name} ì´ë¯¸ ì„¤ì¹˜ë¨\")\n",
        "    except ImportError:\n",
        "        print(f\"âš ï¸ {pip_name} ì„¤ì¹˜ ì¤‘...\")\n",
        "        if pip_name == 'torch':\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"--user\"])\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name, \"--user\"])\n",
        "        print(f\"âœ… {pip_name} ì„¤ì¹˜ ì™„ë£Œ\")\n",
        "\n",
        "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import json\n",
        "    import joblib\n",
        "    from datetime import datetime, timedelta\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "    \n",
        "    # PyTorch ê´€ë ¨\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import Dataset, DataLoader, random_split\n",
        "    \n",
        "    print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì„±ê³µ!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
        "    print(\"ğŸ“‹ ì¬ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\")\n",
        "    raise\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œ í°íŠ¸ ìë™ ê°ì§€)\n",
        "try:\n",
        "    import matplotlib.font_manager as fm\n",
        "    \n",
        "    # macOS ì‹œìŠ¤í…œ í•œê¸€ í°íŠ¸ ì°¾ê¸°\n",
        "    system_fonts = [f.name for f in fm.fontManager.ttflist]\n",
        "    korean_fonts = [f for f in system_fonts if any(keyword in f for keyword in ['Gothic', 'Malgun', 'NanumGothic', 'AppleGothic'])]\n",
        "    \n",
        "    if korean_fonts:\n",
        "        plt.rcParams['font.family'] = korean_fonts[0]\n",
        "        print(f\"âœ… í•œê¸€ í°íŠ¸ ì„¤ì •: {korean_fonts[0]}\")\n",
        "    else:\n",
        "        plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "        print(\"âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©\")\n",
        "        \n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ í°íŠ¸ ì„¤ì • ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}\")\n",
        "    plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# GPU/CPU ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“… í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"ğŸ Python ë²„ì „: {sys.version.split()[0]}\")\n",
        "print(f\"ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}\")\n",
        "print(f\"ğŸ“š ì£¼ìš” íŒ¨í‚¤ì§€ ë²„ì „:\")\n",
        "print(f\"   - pandas: {pd.__version__}\")\n",
        "print(f\"   - numpy: {np.__version__}\")\n",
        "print(f\"   - sklearn: {sklearn.__version__}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "print(\"ğŸ”„ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„° ë¡œë“œ (Task 3.5 ê²°ê³¼)\n",
        "train_path = '../../results/preprocessing/05_data_splitting/train_data_full.csv'\n",
        "train_data = pd.read_csv(train_path)\n",
        "train_data['date'] = pd.to_datetime(train_data['date'])\n",
        "\n",
        "# ì˜ˆì¸¡ í…œí”Œë¦¿ ë¡œë“œ\n",
        "pred_template_path = '../../results/preprocessing/05_data_splitting/prediction_template.csv'\n",
        "pred_template = pd.read_csv(pred_template_path)\n",
        "pred_template['date'] = pd.to_datetime(pred_template['date'])\n",
        "\n",
        "# Lag ì´ˆê¸°ê°’ ë¡œë“œ\n",
        "lag_init_path = '../../results/preprocessing/05_data_splitting/lag_initialization.json'\n",
        "with open(lag_init_path, 'r') as f:\n",
        "    lag_init = json.load(f)\n",
        "\n",
        "# CV í´ë“œ ì •ë³´ ë¡œë“œ\n",
        "cv_metadata_path = '../../results/preprocessing/05_data_splitting/cv_folds_metadata.json'\n",
        "with open(cv_metadata_path, 'r') as f:\n",
        "    cv_metadata = json.load(f)\n",
        "\n",
        "print(f\"âœ… í›ˆë ¨ ë°ì´í„°: {train_data.shape} (ê¸°ê°„: {train_data['date'].min()} ~ {train_data['date'].max()})\")\n",
        "print(f\"âœ… ì˜ˆì¸¡ í…œí”Œë¦¿: {pred_template.shape} (ê¸°ê°„: {pred_template['date'].min()} ~ {pred_template['date'].max()})\")\n",
        "print(f\"âœ… CV ì„¤ì •: {cv_metadata['metadata']['total_folds']}ê°œ í´ë“œ\")\n",
        "\n",
        "# LSTM ì…ë ¥ì„ ìœ„í•œ í”¼ì²˜ ì„ íƒ\n",
        "feature_columns = [col for col in train_data.columns if col not in ['date', 'ìµœëŒ€ì „ë ¥(MW)']]\n",
        "target_column = 'ìµœëŒ€ì „ë ¥(MW)'\n",
        "\n",
        "print(f\"âœ… ì„ íƒëœ í”¼ì²˜ ìˆ˜: {len(feature_columns)}\")\n",
        "print(f\"âœ… íƒ€ê²Ÿ ë³€ìˆ˜: {target_column}\")\n",
        "\n",
        "# ë°ì´í„° ì •ë ¬ (ë‚ ì§œìˆœ)\n",
        "train_data = train_data.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nğŸ“Š íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:\")\n",
        "print(f\"   í‰ê· : {train_data[target_column].mean():.0f} MW\")\n",
        "print(f\"   í‘œì¤€í¸ì°¨: {train_data[target_column].std():.0f} MW\")\n",
        "print(f\"   ë²”ìœ„: {train_data[target_column].min():.0f} ~ {train_data[target_column].max():.0f} MW\")\n",
        "\n",
        "# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì„¤ì •\n",
        "feature_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ\n",
        "features_scaled = feature_scaler.fit_transform(train_data[feature_columns])\n",
        "target_scaled = target_scaler.fit_transform(train_data[[target_column]])\n",
        "\n",
        "print(\"âœ… ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ (MinMaxScaler)\")\n",
        "print(f\"   í”¼ì²˜ ìŠ¤ì¼€ì¼ ë²”ìœ„: [0, 1]\")\n",
        "print(f\"   íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ ë²”ìœ„: [0, 1]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Dataset í´ë˜ìŠ¤ ì •ì˜\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìœ„í•œ Custom Dataset\"\"\"\n",
        "    \n",
        "    def __init__(self, features, targets, sequence_length=60):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: ìŠ¤ì¼€ì¼ëœ í”¼ì²˜ ë°ì´í„° (numpy array)\n",
        "            targets: ìŠ¤ì¼€ì¼ëœ íƒ€ê²Ÿ ë°ì´í„° (numpy array)\n",
        "            sequence_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "        \"\"\"\n",
        "        self.features = features\n",
        "        self.targets = targets.flatten()  # (n, 1) -> (n,)\n",
        "        self.sequence_length = sequence_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.features) - self.sequence_length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # idxë¶€í„° sequence_lengthë§Œí¼ì˜ í”¼ì²˜ ì‹œí€€ìŠ¤\n",
        "        feature_seq = self.features[idx:idx+self.sequence_length]\n",
        "        \n",
        "        # idx+sequence_length ì‹œì ì˜ íƒ€ê²Ÿê°’\n",
        "        target = self.targets[idx+self.sequence_length]\n",
        "        \n",
        "        return torch.FloatTensor(feature_seq), torch.FloatTensor([target])\n",
        "\n",
        "# LSTM ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ì„ ìœ„í•œ LSTM ëª¨ë¸\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size: ì…ë ¥ í”¼ì²˜ ìˆ˜\n",
        "            hidden_size: LSTM ì€ë‹‰ì¸µ í¬ê¸°\n",
        "            num_layers: LSTM ë ˆì´ì–´ ìˆ˜\n",
        "            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
        "        \"\"\"\n",
        "        super(LSTMModel, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # LSTM ë ˆì´ì–´\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        # ì¶œë ¥ ë ˆì´ì–´\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # LSTM ì¶œë ¥\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        \n",
        "        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ ì‚¬ìš©\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        \n",
        "        # ë“œë¡­ì•„ì›ƒ ì ìš©\n",
        "        dropped = self.dropout(last_output)\n",
        "        \n",
        "        # ì„ í˜• ë³€í™˜ìœ¼ë¡œ ìµœì¢… ì¶œë ¥\n",
        "        output = self.fc(dropped)\n",
        "        \n",
        "        return output\n",
        "\n",
        "# ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "SEQUENCE_LENGTH = 60  # 60ì¼ ì‹œí€€ìŠ¤ë¡œ ë‹¤ìŒë‚  ì˜ˆì¸¡\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 100\n",
        "PATIENCE = 10  # Early stopping patience\n",
        "\n",
        "print(\"ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
        "print(f\"   - Sequence Length: {SEQUENCE_LENGTH}\")\n",
        "print(f\"   - Hidden Size: {HIDDEN_SIZE}\")\n",
        "print(f\"   - Num Layers: {NUM_LAYERS}\")\n",
        "print(f\"   - Dropout: {DROPOUT}\")\n",
        "print(f\"   - Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"   - Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"   - Max Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   - Early Stopping Patience: {PATIENCE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í›ˆë ¨ ë° ê²€ì¦ í•¨ìˆ˜ ì •ì˜\n",
        "def train_model(model, train_loader, val_loader, num_epochs, patience=10):\n",
        "    \"\"\"LSTM ëª¨ë¸ í›ˆë ¨\"\"\"\n",
        "    \n",
        "    # ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "    \n",
        "    # í›ˆë ¨ ê¸°ë¡\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "    \n",
        "    print(f\"ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (ë””ë°”ì´ìŠ¤: {device})\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # í›ˆë ¨ ëª¨ë“œ\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        \n",
        "        for batch_features, batch_targets in train_loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            batch_targets = batch_targets.to(device)\n",
        "            \n",
        "            # ìˆœì „íŒŒ\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_targets)\n",
        "            \n",
        "            # ì—­ì „íŒŒ\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # ê²€ì¦ ëª¨ë“œ\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_features, batch_targets in val_loader:\n",
        "                batch_features = batch_features.to(device)\n",
        "                batch_targets = batch_targets.to(device)\n",
        "                \n",
        "                outputs = model(batch_features)\n",
        "                loss = criterion(outputs, batch_targets)\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        # í‰ê·  ì†ì‹¤ ê³„ì‚°\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
        "        scheduler.step(avg_val_loss)\n",
        "        \n",
        "        # Early stopping ì²´í¬\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        # ì§„í–‰ìƒí™© ì¶œë ¥ (10 epochë§ˆë‹¤)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1:3d}/{num_epochs}] | \"\n",
        "                  f\"Train Loss: {avg_train_loss:.6f} | \"\n",
        "                  f\"Val Loss: {avg_val_loss:.6f} | \"\n",
        "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nâ¹ï¸ Early stopping at epoch {epoch+1}\")\n",
        "            print(f\"   Best validation loss: {best_val_loss:.6f}\")\n",
        "            break\n",
        "    \n",
        "    # ìµœê³  ëª¨ë¸ ë³µì›\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë³µì› (Val Loss: {best_val_loss:.6f})\")\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "\n",
        "def calculate_rmse(y_true, y_pred):\n",
        "    \"\"\"RMSE ê³„ì‚°\"\"\"\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print(\"ğŸ”§ í›ˆë ¨ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‹œê³„ì—´ êµì°¨ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨\n",
        "print(\"ğŸ”„ ì‹œê³„ì—´ êµì°¨ê²€ì¦ ì‹œì‘...\")\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥ìš©\n",
        "cv_results = []\n",
        "models_dict = {}\n",
        "\n",
        "# ê° í´ë“œë³„ í›ˆë ¨\n",
        "for fold_id, fold_info in cv_metadata['folds'].items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ğŸ”„ {fold_id} í›ˆë ¨ ì‹œì‘\")\n",
        "    print(f\"   í›ˆë ¨: {fold_info['train_size']}ì¼, ê²€ì¦: {fold_info['val_size']}ì¼\")\n",
        "    print(f\"   í›ˆë ¨ ê¸°ê°„: {fold_info['train_start']} ~ {fold_info['train_end']}\")\n",
        "    print(f\"   ê²€ì¦ ê¸°ê°„: {fold_info['val_start']} ~ {fold_info['val_end']}\")\n",
        "    \n",
        "    # í´ë“œë³„ ë°ì´í„° ë¶„í• \n",
        "    train_start_idx = 0\n",
        "    train_end_idx = fold_info['train_size']\n",
        "    val_start_idx = train_end_idx\n",
        "    val_end_idx = val_start_idx + fold_info['val_size']\n",
        "    \n",
        "    # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ì¤€ë¹„\n",
        "    train_features = features_scaled[:train_end_idx]\n",
        "    train_targets = target_scaled[:train_end_idx]\n",
        "    val_features = features_scaled[val_start_idx:val_end_idx]\n",
        "    val_targets = target_scaled[val_start_idx:val_end_idx]\n",
        "    \n",
        "    # Dataset ë° DataLoader ìƒì„±\n",
        "    train_dataset = TimeSeriesDataset(train_features, train_targets, SEQUENCE_LENGTH)\n",
        "    val_dataset = TimeSeriesDataset(val_features, val_targets, SEQUENCE_LENGTH)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "    print(f\"   ë°ì´í„°ì…‹ í¬ê¸°: í›ˆë ¨ {len(train_dataset)}, ê²€ì¦ {len(val_dataset)}\")\n",
        "    \n",
        "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
        "    input_size = len(feature_columns)\n",
        "    model = LSTMModel(\n",
        "        input_size=input_size,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    \n",
        "    print(f\"   ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    # ëª¨ë¸ í›ˆë ¨\n",
        "    train_losses, val_losses = train_model(\n",
        "        model, train_loader, val_loader, NUM_EPOCHS, PATIENCE\n",
        "    )\n",
        "    \n",
        "    # ê²€ì¦ ë°ì´í„°ë¡œ RMSE ê³„ì‚°\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_actuals = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_targets in val_loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            outputs = model(batch_features)\n",
        "            \n",
        "            # CPUë¡œ ì´ë™ ë° ìŠ¤ì¼€ì¼ ë³µì›\n",
        "            pred_scaled = outputs.cpu().numpy()\n",
        "            target_scaled_batch = batch_targets.cpu().numpy()\n",
        "            \n",
        "            pred_original = target_scaler.inverse_transform(pred_scaled)\n",
        "            target_original = target_scaler.inverse_transform(target_scaled_batch)\n",
        "            \n",
        "            val_predictions.extend(pred_original.flatten())\n",
        "            val_actuals.extend(target_original.flatten())\n",
        "    \n",
        "    # RMSE ê³„ì‚°\n",
        "    fold_rmse = calculate_rmse(val_actuals, val_predictions)\n",
        "    \n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    fold_result = {\n",
        "        'fold': fold_id,\n",
        "        'train_size': fold_info['train_size'],\n",
        "        'val_size': fold_info['val_size'],\n",
        "        'val_rmse': fold_rmse,\n",
        "        'final_train_loss': train_losses[-1],\n",
        "        'final_val_loss': val_losses[-1],\n",
        "        'epochs_trained': len(train_losses)\n",
        "    }\n",
        "    cv_results.append(fold_result)\n",
        "    models_dict[fold_id] = model.state_dict().copy()\n",
        "    \n",
        "    print(f\"âœ… {fold_id} ì™„ë£Œ!\")\n",
        "    print(f\"   ê²€ì¦ RMSE: {fold_rmse:.2f} MW\")\n",
        "    print(f\"   í›ˆë ¨ ì™„ë£Œ ì—í¬í¬: {len(train_losses)}\")\n",
        "\n",
        "# êµì°¨ê²€ì¦ ê²°ê³¼ ìš”ì•½\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ğŸ“Š êµì°¨ê²€ì¦ ê²°ê³¼ ìš”ì•½\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "cv_rmses = [result['val_rmse'] for result in cv_results]\n",
        "print(f\"í‰ê·  RMSE: {np.mean(cv_rmses):.2f} Â± {np.std(cv_rmses):.2f} MW\")\n",
        "print(f\"ìµœê³  RMSE: {min(cv_rmses):.2f} MW\")\n",
        "print(f\"ìµœì•… RMSE: {max(cv_rmses):.2f} MW\")\n",
        "\n",
        "print(f\"\\ní´ë“œë³„ ìƒì„¸ ê²°ê³¼:\")\n",
        "for result in cv_results:\n",
        "    print(f\"  {result['fold']}: {result['val_rmse']:.2f} MW \"\n",
        "          f\"({result['epochs_trained']} epochs)\")\n",
        "\n",
        "print(\"âœ… êµì°¨ê²€ì¦ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì „ì²´ ë°ì´í„°)\n",
        "print(\"ğŸ¯ ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì „ì²´ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©)\")\n",
        "\n",
        "# ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ Dataset ìƒì„±\n",
        "full_dataset = TimeSeriesDataset(features_scaled, target_scaled, SEQUENCE_LENGTH)\n",
        "\n",
        "# 80-20 ë¶„í•  (í›ˆë ¨-ê²€ì¦)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"ì „ì²´ ë°ì´í„°ì…‹: {len(full_dataset)} samples\")\n",
        "print(f\"í›ˆë ¨: {len(train_dataset)}, ê²€ì¦: {len(val_dataset)}\")\n",
        "\n",
        "# ìµœì¢… ëª¨ë¸ ì´ˆê¸°í™”\n",
        "final_model = LSTMModel(\n",
        "    input_size=len(feature_columns),\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "print(f\"ìµœì¢… ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in final_model.parameters()):,}\")\n",
        "\n",
        "# ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
        "print(\"\\nğŸš€ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "final_train_losses, final_val_losses = train_model(\n",
        "    final_model, train_loader, val_loader, NUM_EPOCHS, PATIENCE\n",
        ")\n",
        "\n",
        "print(\"âœ… ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 527ì¼ ì˜ˆì¸¡ ìƒì„±\n",
        "print(\"ğŸ”® 527ì¼ ì˜ˆì¸¡ ìƒì„± ì¤‘...\")\n",
        "\n",
        "def generate_future_predictions(model, last_sequence, pred_template, feature_scaler, target_scaler, sequence_length):\n",
        "    \"\"\"527ì¼ ë¯¸ë˜ ì˜ˆì¸¡ ìƒì„±\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    \n",
        "    # ì´ˆê¸° ì‹œí€€ìŠ¤ ì„¤ì • (ë§ˆì§€ë§‰ sequence_lengthì¼ì˜ í”¼ì²˜)\n",
        "    current_sequence = last_sequence.copy()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for day_idx in range(len(pred_template)):\n",
        "            # í˜„ì¬ ì‹œí€€ìŠ¤ë¥¼ í…ì„œë¡œ ë³€í™˜\n",
        "            seq_tensor = torch.FloatTensor(current_sequence).unsqueeze(0).to(device)\n",
        "            \n",
        "            # ì˜ˆì¸¡\n",
        "            pred_scaled = model(seq_tensor).cpu().numpy()\n",
        "            pred_original = target_scaler.inverse_transform(pred_scaled)[0, 0]\n",
        "            predictions.append(pred_original)\n",
        "            \n",
        "            # ë‹¤ìŒ ë‚ ì˜ í”¼ì²˜ ê°€ì ¸ì˜¤ê¸° (ì˜ˆì¸¡ í…œí”Œë¦¿ì—ì„œ)\n",
        "            if day_idx < len(pred_template) - 1:\n",
        "                next_day_features = pred_template.iloc[day_idx][feature_columns].values\n",
        "                next_day_features_scaled = feature_scaler.transform([next_day_features])[0]\n",
        "                \n",
        "                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ (ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°, ìƒˆë¡œìš´ ê²ƒ ì¶”ê°€)\n",
        "                current_sequence = np.vstack([current_sequence[1:], next_day_features_scaled])\n",
        "    \n",
        "    return np.array(predictions)\n",
        "\n",
        "# ë§ˆì§€ë§‰ sequence_lengthì¼ì˜ í”¼ì²˜ ì¶”ì¶œ\n",
        "last_sequence = features_scaled[-SEQUENCE_LENGTH:]\n",
        "\n",
        "# 527ì¼ ì˜ˆì¸¡ ìƒì„±\n",
        "future_predictions = generate_future_predictions(\n",
        "    final_model, last_sequence, pred_template, \n",
        "    feature_scaler, target_scaler, SEQUENCE_LENGTH\n",
        ")\n",
        "\n",
        "print(f\"âœ… 527ì¼ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
        "print(f\"   ì˜ˆì¸¡ ë²”ìœ„: {future_predictions.min():.0f} ~ {future_predictions.max():.0f} MW\")\n",
        "print(f\"   ì˜ˆì¸¡ í‰ê· : {future_predictions.mean():.0f} MW\")\n",
        "\n",
        "# ëŒ€íšŒ í˜•ì‹ ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜\n",
        "def format_competition_date(date):\n",
        "    \"\"\"YYYY.M.D í˜•ì‹ìœ¼ë¡œ ë‚ ì§œ ë³€í™˜ (í•œ ìë¦¬ ìˆ«ìì¼ ë•Œ 0 ìƒëµ)\"\"\"\n",
        "    return f\"{date.year}.{date.month}.{date.day}\"\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "submission = pd.DataFrame({\n",
        "    'date': [format_competition_date(date) for date in pred_template['date']],\n",
        "    'ìµœëŒ€ì „ë ¥(MW)': future_predictions\n",
        "})\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "submission_path = 'submission_lstm.csv'\n",
        "submission.to_csv(submission_path, index=False)\n",
        "print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: {submission_path}\")\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°\n",
        "print(f\"\\nğŸ“‹ ì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:\")\n",
        "print(submission.head(10))\n",
        "print(\"...\")\n",
        "print(submission.tail(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê²°ê³¼ ì‹œê°í™” ë° ëª¨ë¸ ì €ì¥\n",
        "print(\"ğŸ“Š ê²°ê³¼ ì‹œê°í™” ë° ëª¨ë¸ ì €ì¥...\")\n",
        "\n",
        "# 1. í›ˆë ¨ ì†ì‹¤ ì‹œê°í™”\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# ì²« ë²ˆì§¸ ì„œë¸Œí”Œë¡¯: ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì†ì‹¤\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(final_train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(final_val_losses, label='Validation Loss', color='red')\n",
        "plt.title('Final Model Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# ë‘ ë²ˆì§¸ ì„œë¸Œí”Œë¡¯: CV RMSE ë¶„í¬\n",
        "plt.subplot(2, 3, 2)\n",
        "cv_rmses = [result['val_rmse'] for result in cv_results]\n",
        "plt.bar(range(len(cv_rmses)), cv_rmses, color='skyblue', edgecolor='navy')\n",
        "plt.title('Cross-Validation RMSE by Fold')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('RMSE (MW)')\n",
        "plt.xticks(range(len(cv_rmses)), [f'Fold {i+1}' for i in range(len(cv_rmses))])\n",
        "for i, rmse in enumerate(cv_rmses):\n",
        "    plt.text(i, rmse + 50, f'{rmse:.0f}', ha='center', va='bottom')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# ì„¸ ë²ˆì§¸ ì„œë¸Œí”Œë¡¯: ì˜ˆì¸¡ê°’ ë¶„í¬\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.hist(future_predictions, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "plt.title('Prediction Distribution')\n",
        "plt.xlabel('Power Demand (MW)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(future_predictions.mean(), color='red', linestyle='--', \n",
        "           label=f'Mean: {future_predictions.mean():.0f} MW')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# ë„¤ ë²ˆì§¸ ì„œë¸Œí”Œë¡¯: ì›”ë³„ ì˜ˆì¸¡ íŒ¨í„´\n",
        "plt.subplot(2, 3, 4)\n",
        "pred_dates = pred_template['date']\n",
        "monthly_avg = []\n",
        "months = []\n",
        "for month in range(1, 13):\n",
        "    month_mask = pred_dates.dt.month == month\n",
        "    if month_mask.any():\n",
        "        monthly_avg.append(future_predictions[month_mask].mean())\n",
        "        months.append(month)\n",
        "\n",
        "plt.plot(months, monthly_avg, marker='o', linewidth=2, markersize=6)\n",
        "plt.title('Monthly Average Predictions')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Power Demand (MW)')\n",
        "plt.xticks(months)\n",
        "plt.grid(True)\n",
        "\n",
        "# ë‹¤ì„¯ ë²ˆì§¸ ì„œë¸Œí”Œë¡¯: ì‹œê³„ì—´ ì˜ˆì¸¡ ê·¸ë˜í”„ (ì²« 90ì¼)\n",
        "plt.subplot(2, 3, 5)\n",
        "first_90_days = pred_dates[:90]\n",
        "first_90_predictions = future_predictions[:90]\n",
        "plt.plot(first_90_days, first_90_predictions, linewidth=1.5, color='purple')\n",
        "plt.title('First 90 Days Predictions')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Power Demand (MW)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "# ì—¬ì„¯ ë²ˆì§¸ ì„œë¸Œí”Œë¡¯: ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.axis('off')\n",
        "summary_text = f'''\n",
        "LSTM Model Performance Summary\n",
        "\n",
        "Cross-Validation Results:\n",
        "  Mean RMSE: {np.mean(cv_rmses):.1f} Â± {np.std(cv_rmses):.1f} MW\n",
        "  Best RMSE: {min(cv_rmses):.1f} MW\n",
        "  Worst RMSE: {max(cv_rmses):.1f} MW\n",
        "\n",
        "Model Architecture:\n",
        "  Input Features: {len(feature_columns)}\n",
        "  Hidden Size: {HIDDEN_SIZE}\n",
        "  Num Layers: {NUM_LAYERS}\n",
        "  Sequence Length: {SEQUENCE_LENGTH}\n",
        "  Parameters: {sum(p.numel() for p in final_model.parameters()):,}\n",
        "\n",
        "Prediction Stats:\n",
        "  Mean: {future_predictions.mean():.0f} MW\n",
        "  Std: {future_predictions.std():.0f} MW\n",
        "  Range: {future_predictions.min():.0f} - {future_predictions.max():.0f} MW\n",
        "'''\n",
        "plt.text(0.05, 0.95, summary_text, fontsize=10, verticalalignment='top',\n",
        "         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('lstm_model_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
        "print(\"\\nğŸ’¾ ëª¨ë¸ ë° ë©”íƒ€ë°ì´í„° ì €ì¥...\")\n",
        "\n",
        "# ëª¨ë¸ ì €ì¥\n",
        "torch.save({\n",
        "    'model_state_dict': final_model.state_dict(),\n",
        "    'model_architecture': {\n",
        "        'input_size': len(feature_columns),\n",
        "        'hidden_size': HIDDEN_SIZE,\n",
        "        'num_layers': NUM_LAYERS,\n",
        "        'dropout': DROPOUT\n",
        "    },\n",
        "    'training_config': {\n",
        "        'sequence_length': SEQUENCE_LENGTH,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'num_epochs': NUM_EPOCHS,\n",
        "        'patience': PATIENCE\n",
        "    },\n",
        "    'feature_columns': feature_columns,\n",
        "    'target_column': target_column\n",
        "}, 'lstm_model.pth')\n",
        "\n",
        "# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
        "joblib.dump(feature_scaler, 'feature_scaler_lstm.pkl')\n",
        "joblib.dump(target_scaler, 'target_scaler_lstm.pkl')\n",
        "\n",
        "# ê²°ê³¼ ë©”íƒ€ë°ì´í„° ì €ì¥\n",
        "metadata = {\n",
        "    'model_type': 'LSTM',\n",
        "    'creation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'cv_results': cv_results,\n",
        "    'final_model_performance': {\n",
        "        'final_train_loss': final_train_losses[-1],\n",
        "        'final_val_loss': final_val_losses[-1],\n",
        "        'epochs_trained': len(final_train_losses)\n",
        "    },\n",
        "    'prediction_stats': {\n",
        "        'mean': float(future_predictions.mean()),\n",
        "        'std': float(future_predictions.std()),\n",
        "        'min': float(future_predictions.min()),\n",
        "        'max': float(future_predictions.max())\n",
        "    },\n",
        "    'hyperparameters': {\n",
        "        'sequence_length': SEQUENCE_LENGTH,\n",
        "        'hidden_size': HIDDEN_SIZE,\n",
        "        'num_layers': NUM_LAYERS,\n",
        "        'dropout': DROPOUT,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('lstm_model_metadata.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"âœ… ì €ì¥ ì™„ë£Œ!\")\n",
        "print(\"   - lstm_model.pth: ëª¨ë¸ ê°€ì¤‘ì¹˜\")\n",
        "print(\"   - feature_scaler_lstm.pkl: í”¼ì²˜ ìŠ¤ì¼€ì¼ëŸ¬\")\n",
        "print(\"   - target_scaler_lstm.pkl: íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ëŸ¬\")\n",
        "print(\"   - lstm_model_metadata.json: ëª¨ë¸ ë©”íƒ€ë°ì´í„°\")\n",
        "print(\"   - submission_lstm.csv: ëŒ€íšŒ ì œì¶œ íŒŒì¼\")\n",
        "print(\"   - lstm_model_analysis.png: ë¶„ì„ ì‹œê°í™”\")\n",
        "\n",
        "print(f\"\\nğŸ¯ Task 5 ì™„ë£Œ!\")\n",
        "print(f\"   í‰ê·  CV RMSE: {np.mean(cv_rmses):.1f} MW\")\n",
        "print(f\"   ì œì¶œ íŒŒì¼: submission_lstm.csv ({len(submission)} ì˜ˆì¸¡)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
