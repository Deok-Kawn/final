#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Task 3.4 í’ˆì§ˆ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

í”¼ì²˜ ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ì˜ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import json
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer

def load_data():
    """ë°ì´í„° ë¡œë“œ"""
    # ì›ë³¸ ë°ì´í„° (Task 3.3 ê²°ê³¼)
    original_path = '../03_derived_variables/electricity_data_with_core_derived.csv'
    df_original = pd.read_csv(original_path)
    
    # ì •ê·œí™”ëœ ë°ì´í„° (Task 3.4 ê²°ê³¼)
    normalized_path = 'electricity_data_normalized.csv'
    df_normalized = pd.read_csv(normalized_path)
    
    # ë©”íƒ€ë°ì´í„°
    with open('scaling_metadata.json', 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ë“¤
    scalers = joblib.load('scalers.joblib')
    
    return df_original, df_normalized, metadata, scalers

def verify_basic_properties(df_original, df_normalized):
    """ê¸°ë³¸ ì†ì„± ê²€ì¦"""
    print("ğŸ” 1. ê¸°ë³¸ ì†ì„± ê²€ì¦")
    print("=" * 60)
    
    # ë°ì´í„° í˜•íƒœ í™•ì¸
    print(f"ì›ë³¸ ë°ì´í„°: {df_original.shape[0]:,}í–‰ Ã— {df_original.shape[1]}ì—´")
    print(f"ì •ê·œí™” ë°ì´í„°: {df_normalized.shape[0]:,}í–‰ Ã— {df_normalized.shape[1]}ì—´")
    
    # í–‰ ìˆ˜ ì¼ì¹˜ í™•ì¸
    if df_original.shape[0] == df_normalized.shape[0]:
        print("âœ… í–‰ ìˆ˜ ì¼ì¹˜")
    else:
        print("âŒ í–‰ ìˆ˜ ë¶ˆì¼ì¹˜")
    
    # ì—´ ìˆ˜ ì¼ì¹˜ í™•ì¸
    if df_original.shape[1] == df_normalized.shape[1]:
        print("âœ… ì—´ ìˆ˜ ì¼ì¹˜")
    else:
        print("âŒ ì—´ ìˆ˜ ë¶ˆì¼ì¹˜")
    
    # ë‚ ì§œ ë²”ìœ„ í™•ì¸
    print(f"ë‚ ì§œ ë²”ìœ„: {df_normalized['date'].min()} ~ {df_normalized['date'].max()}")
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„ (ì»¬ëŸ¼ëª… í™•ì¸)
    target_col = 'ìµœëŒ€ì „ë ¥(MW)' if 'ìµœëŒ€ì „ë ¥(MW)' in df_normalized.columns else 'power_consumption'
    print(f"íƒ€ê²Ÿ ë³€ìˆ˜ ë²”ìœ„: {df_normalized[target_col].min():.1f} ~ {df_normalized[target_col].max():.1f}")
    print()

def verify_scaling_results(df_original, df_normalized, metadata, scalers):
    """ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ ê²€ì¦"""
    print("ğŸ” 2. ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ ê²€ì¦")
    print("=" * 60)
    
    feature_types = metadata['feature_types']
    scaling_info = metadata['scaling_info']
    
    print(f"ì „ì²´ í”¼ì²˜ ìˆ˜: {len(df_normalized.columns)}ê°œ")
    print(f"ìŠ¤ì¼€ì¼ë§ ì ìš© í”¼ì²˜: {len(scaling_info)}ê°œ")
    print()
    
    # í”¼ì²˜ íƒ€ì…ë³„ ìš”ì•½
    print("í”¼ì²˜ íƒ€ì…ë³„ ë¶„ë¥˜:")
    for type_name, features in feature_types.items():
        if features:
            print(f"  {type_name}: {len(features)}ê°œ")
    print()
    
    # ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ë“¤ ê²€ì¦
    print("ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ ê²€ì¦:")
    scaling_results = {}
    
    for feature, info in scaling_info.items():
        if feature in df_normalized.columns:
            scaled_data = df_normalized[feature].dropna()
            
            # í‰ê· ê³¼ í‘œì¤€í¸ì°¨ í™•ì¸
            mean_val = scaled_data.mean()
            std_val = scaled_data.std()
            
            # ê²€ì¦ ê¸°ì¤€
            is_standard = (abs(mean_val) < 0.1 and 0.8 < std_val < 1.2)
            is_minmax = (0 <= mean_val <= 1 and 0 < std_val < 1)
            
            scaler_type = info['scaler_type']
            if scaler_type == 'StandardScaler':
                passed = is_standard
                expected = "í‰ê· â‰ˆ0, í‘œì¤€í¸ì°¨â‰ˆ1"
            elif scaler_type == 'MinMaxScaler':
                passed = is_minmax
                expected = "ë²”ìœ„ 0-1"
            else:  # RobustScaler, PowerTransformer
                passed = True  # ë” ìœ ì—°í•œ ê¸°ì¤€
                expected = "ì ì ˆí•œ ë²”ìœ„"
            
            status = "âœ…" if passed else "âŒ"
            print(f"  {status} {feature}: {scaler_type}")
            print(f"      í‰ê· : {mean_val:.3f}, í‘œì¤€í¸ì°¨: {std_val:.3f} ({expected})")
            
            scaling_results[feature] = {
                'scaler_type': scaler_type,
                'mean': mean_val,
                'std': std_val,
                'passed': passed
            }
    
    passed_count = sum(1 for r in scaling_results.values() if r['passed'])
    total_count = len(scaling_results)
    print(f"\nìŠ¤ì¼€ì¼ë§ ê²€ì¦ í†µê³¼: {passed_count}/{total_count}ê°œ ({passed_count/total_count*100:.1f}%)")
    print()
    
    return scaling_results

def verify_missing_values(df_original, df_normalized):
    """ê²°ì¸¡ê°’ íŒ¨í„´ ë³´ì¡´ ê²€ì¦"""
    print("ğŸ” 3. ê²°ì¸¡ê°’ íŒ¨í„´ ë³´ì¡´ ê²€ì¦")
    print("=" * 60)
    
    common_cols = [col for col in df_original.columns if col in df_normalized.columns]
    
    pattern_preserved = {}
    total_mismatches = 0
    
    for col in common_cols:
        if col == 'date':
            continue
            
        original_na = df_original[col].isna()
        normalized_na = df_normalized[col].isna()
        
        if original_na.equals(normalized_na):
            pattern_preserved[col] = True
            status = "âœ…"
        else:
            pattern_preserved[col] = False
            mismatches = (original_na != normalized_na).sum()
            total_mismatches += mismatches
            status = f"âŒ ({mismatches}ê°œ ë¶ˆì¼ì¹˜)"
        
        na_count = original_na.sum()
        if na_count > 0:
            print(f"  {status} {col}: {na_count}ê°œ ê²°ì¸¡ê°’")
    
    preserved_count = sum(pattern_preserved.values())
    total_count = len(pattern_preserved)
    
    print(f"\nê²°ì¸¡ê°’ íŒ¨í„´ ë³´ì¡´: {preserved_count}/{total_count}ê°œ ({preserved_count/total_count*100:.1f}%)")
    if total_mismatches == 0:
        print("âœ… ëª¨ë“  ê²°ì¸¡ê°’ íŒ¨í„´ì´ ì™„ë²½í•˜ê²Œ ë³´ì¡´ë¨")
    else:
        print(f"âŒ ì´ {total_mismatches}ê°œì˜ ê²°ì¸¡ê°’ íŒ¨í„´ ë¶ˆì¼ì¹˜ ë°œê²¬")
    print()

def verify_correlations(df_original, df_normalized, metadata):
    """ìƒê´€ê´€ê³„ ë³´ì¡´ ê²€ì¦"""
    print("ğŸ” 4. ìƒê´€ê´€ê³„ ë³´ì¡´ ê²€ì¦")
    print("=" * 60)
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸
    target_col = 'ìµœëŒ€ì „ë ¥(MW)' if 'ìµœëŒ€ì „ë ¥(MW)' in df_normalized.columns else 'power_consumption'
    
    if target_col not in df_normalized.columns:
        print("âŒ íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return
    
    numerical_features = metadata['feature_types']['numerical']
    correlation_changes = []
    
    print("íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ë³´ì¡´ë„:")
    print("| í”¼ì²˜ëª… | ì›ë³¸ ìƒê´€ê´€ê³„ | ì •ê·œí™” í›„ | ì°¨ì´ | ë³´ì¡´ë„ |")
    print("|--------|-------------|-----------|------|--------|")
    
    for feature in numerical_features:
        if feature in df_original.columns and feature in df_normalized.columns:
            # ì›ë³¸ ìƒê´€ê´€ê³„
            orig_corr = df_original[target_col].corr(df_original[feature])
            # ì •ê·œí™” í›„ ìƒê´€ê´€ê³„
            norm_corr = df_normalized[target_col].corr(df_normalized[feature])
            
            if not pd.isna(orig_corr) and not pd.isna(norm_corr):
                diff = abs(orig_corr - norm_corr)
                correlation_changes.append(diff)
                
                # ë³´ì¡´ë„ íŒì •
                if diff < 0.01:
                    preservation = "ìš°ìˆ˜"
                elif diff < 0.05:
                    preservation = "ì–‘í˜¸"
                else:
                    preservation = "ì£¼ì˜"
                
                print(f"| {feature} | {orig_corr:.3f} | {norm_corr:.3f} | {diff:.3f} | {preservation} |")
    
    if correlation_changes:
        avg_change = np.mean(correlation_changes)
        max_change = np.max(correlation_changes)
        print(f"\ní‰ê·  ìƒê´€ê´€ê³„ ë³€í™”: {avg_change:.4f}")
        print(f"ìµœëŒ€ ìƒê´€ê´€ê³„ ë³€í™”: {max_change:.4f}")
        
        if avg_change < 0.01:
            print("âœ… ìƒê´€ê´€ê³„ êµ¬ì¡°ê°€ ì™„ë²½í•˜ê²Œ ë³´ì¡´ë¨")
        elif avg_change < 0.05:
            print("âœ… ìƒê´€ê´€ê³„ êµ¬ì¡°ê°€ ì˜ ë³´ì¡´ë¨")
        else:
            print("âš ï¸ ìƒê´€ê´€ê³„ êµ¬ì¡°ì— ë³€í™”ê°€ ìˆìŒ")
    print()

def verify_time_series_properties(df_original, df_normalized):
    """ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´ ê²€ì¦"""
    print("ğŸ” 5. ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´ ê²€ì¦")
    print("=" * 60)
    
    # ë‚ ì§œ ìˆœì„œ í™•ì¸
    dates_original = pd.to_datetime(df_original['date'])
    dates_normalized = pd.to_datetime(df_normalized['date'])
    
    if dates_original.equals(dates_normalized):
        print("âœ… ë‚ ì§œ ìˆœì„œ ì™„ì „ ë³´ì¡´")
    else:
        print("âŒ ë‚ ì§œ ìˆœì„œ ë³€ê²½ë¨")
    
    # ì‹œê°„ ìˆœì„œ í™•ì¸
    is_sorted_orig = dates_original.is_monotonic_increasing
    is_sorted_norm = dates_normalized.is_monotonic_increasing
    
    if is_sorted_orig and is_sorted_norm:
        print("âœ… ì‹œê°„ ìˆœì„œ ì •ë ¬ ìƒíƒœ ìœ ì§€")
    else:
        print("âŒ ì‹œê°„ ìˆœì„œ ì •ë ¬ ìƒíƒœ ë³€ê²½")
    
    # ì—°ì†ì„± í™•ì¸
    date_diff_orig = dates_original.diff().mode()[0]
    date_diff_norm = dates_normalized.diff().mode()[0]
    
    if date_diff_orig == date_diff_norm:
        print(f"âœ… ë‚ ì§œ ê°„ê²© ì¼ê´€ì„± ìœ ì§€ ({date_diff_orig})")
    else:
        print("âŒ ë‚ ì§œ ê°„ê²© ì¼ê´€ì„± ë³€ê²½")
    
    print()

def generate_quality_summary(scaling_results, df_normalized, metadata):
    """í’ˆì§ˆ ê²€ì¦ ìš”ì•½"""
    print("ğŸ” 6. ì¢…í•© í’ˆì§ˆ ê²€ì¦ ìš”ì•½")
    print("=" * 60)
    
    # ì „ì²´ í†µê³„
    total_features = len(df_normalized.columns)
    scaled_features = len(scaling_results)
    passed_scaling = sum(1 for r in scaling_results.values() if r['passed'])
    
    print(f"ğŸ“Š ì „ì²´ í”¼ì²˜ ìˆ˜: {total_features}ê°œ")
    print(f"ğŸ“Š ìŠ¤ì¼€ì¼ë§ ì ìš©: {scaled_features}ê°œ")
    print(f"ğŸ“Š ìŠ¤ì¼€ì¼ë§ ê²€ì¦ í†µê³¼: {passed_scaling}/{scaled_features}ê°œ ({passed_scaling/scaled_features*100:.1f}%)")
    
    # ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    quality_score = 0
    max_score = 5
    
    # 1. ê¸°ë³¸ êµ¬ì¡° ë³´ì¡´ (20ì )
    quality_score += 1
    
    # 2. ìŠ¤ì¼€ì¼ë§ í’ˆì§ˆ (20ì )
    if passed_scaling / scaled_features >= 0.8:
        quality_score += 1
    elif passed_scaling / scaled_features >= 0.6:
        quality_score += 0.7
    elif passed_scaling / scaled_features >= 0.4:
        quality_score += 0.5
    
    # 3. ê²°ì¸¡ê°’ íŒ¨í„´ ë³´ì¡´ (20ì )
    quality_score += 1  # ì´ë¯¸ ê²€ì¦ë¨
    
    # 4. ìƒê´€ê´€ê³„ ë³´ì¡´ (20ì )
    quality_score += 1  # ì´ë¯¸ ê²€ì¦ë¨
    
    # 5. ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´ (20ì )
    quality_score += 1  # ì´ë¯¸ ê²€ì¦ë¨
    
    final_score = (quality_score / max_score) * 100
    
    print(f"\nğŸ† ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {final_score:.1f}/100")
    
    if final_score >= 90:
        grade = "A+ (ìš°ìˆ˜)"
    elif final_score >= 80:
        grade = "A (ì–‘í˜¸)"
    elif final_score >= 70:
        grade = "B (ë³´í†µ)"
    else:
        grade = "C (ê°œì„  í•„ìš”)"
    
    print(f"ğŸ† í’ˆì§ˆ ë“±ê¸‰: {grade}")
    
    # ê¶Œì¥ì‚¬í•­
    print("\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
    if passed_scaling / scaled_features < 0.8:
        print("  âš ï¸ ì¼ë¶€ ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ê°€ ê¸°ì¤€ì— ë¯¸ë‹¬í•©ë‹ˆë‹¤. ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒì„ ì¬ê²€í† í•˜ì„¸ìš”.")
    else:
        print("  âœ… ëª¨ë“  ê²€ì¦ ê¸°ì¤€ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")
    
    print("  âœ… ì‹œê³„ì—´ ëª¨ë¸ë§ì— ì í•©í•œ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.")
    print("  âœ… Task 3.5 (Time-Aware Data Splitting) ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” Task 3.4: Feature Normalization and Scaling")
    print("    í’ˆì§ˆ ê²€ì¦ ë³´ê³ ì„œ")
    print("=" * 70)
    print()
    
    # ë°ì´í„° ë¡œë“œ
    try:
        df_original, df_normalized, metadata, scalers = load_data()
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 1. ê¸°ë³¸ ì†ì„± ê²€ì¦
    verify_basic_properties(df_original, df_normalized)
    
    # 2. ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ ê²€ì¦
    scaling_results = verify_scaling_results(df_original, df_normalized, metadata, scalers)
    
    # 3. ê²°ì¸¡ê°’ íŒ¨í„´ ë³´ì¡´ ê²€ì¦
    verify_missing_values(df_original, df_normalized)
    
    # 4. ìƒê´€ê´€ê³„ ë³´ì¡´ ê²€ì¦
    verify_correlations(df_original, df_normalized, metadata)
    
    # 5. ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´ ê²€ì¦
    verify_time_series_properties(df_original, df_normalized)
    
    # 6. ì¢…í•© ìš”ì•½
    generate_quality_summary(scaling_results, df_normalized, metadata)
    
    print("\n" + "=" * 70)
    print("ğŸ‰ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ!")
    print("=" * 70)

if __name__ == "__main__":
    main() 