#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Task 3.4: Feature Normalization and Scaling

í•œêµ­ ì „ë ¥ ìˆ˜ìš” ì‹œê³„ì—´ ë°ì´í„°ì˜ í”¼ì²˜ë“¤ì„ ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤.
ì‹œê³„ì—´ íŠ¹ì„±ì„ ë³´ì¡´í•˜ë©´ì„œ ëª¨ë¸ í•™ìŠµì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer
import joblib
import warnings
import json
import os
from datetime import datetime

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['Malgun Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

def analyze_feature_types(df):
    """í”¼ì²˜ íƒ€ì…ì„ ë¶„ì„í•˜ê³  ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    
    feature_types = {
        'target': [],
        'datetime': [],
        'categorical': [],
        'binary': [],
        'numerical': [],
        'cyclical': [],
        'skip': []  # ì´ë¯¸ ì •ê·œí™”ëœ í”¼ì²˜ë“¤
    }
    
    for col in df.columns:
        if col == 'power_consumption':
            feature_types['target'].append(col)
        elif col == 'date':
            feature_types['datetime'].append(col)
        elif col.endswith('_name') or col == 'holiday_type':
            feature_types['categorical'].append(col)
        elif col.startswith('is_') or col in ['weekday_name', 'month_name']:
            feature_types['binary'].append(col)
        elif col.endswith('_sin') or col.endswith('_cos'):
            feature_types['cyclical'].append(col)
        elif col in ['year', 'month', 'quarter', 'dayofweek', 'dayofyear', 'weekofyear', 'season']:
            feature_types['numerical'].append(col)
        elif col.startswith('lag_') or col.startswith('rolling_') or col == 'daily_change':
            feature_types['numerical'].append(col)
        else:
            feature_types['skip'].append(col)
    
    return feature_types

def get_optimal_scaler(data, feature_name):
    """í”¼ì²˜ì˜ ë¶„í¬ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ìµœì  ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
    
    # ê²°ì¸¡ê°’ ì œê±°í•˜ê³  ë¶„ì„
    clean_data = data.dropna()
    
    if len(clean_data) == 0:
        return None, "No valid data"
    
    # ê¸°ë³¸ í†µê³„ëŸ‰
    skewness = abs(clean_data.skew())
    
    # IQRì„ ì´ìš©í•œ ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°
    Q1 = clean_data.quantile(0.25)
    Q3 = clean_data.quantile(0.75)
    IQR = Q3 - Q1
    outliers = ((clean_data < (Q1 - 1.5 * IQR)) | (clean_data > (Q3 + 1.5 * IQR))).sum()
    outlier_ratio = outliers / len(clean_data)
    
    # ë¶„í¬ì˜ ë²”ìœ„
    data_range = clean_data.max() - clean_data.min()
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ ë¡œì§
    if outlier_ratio > 0.1:  # ì´ìƒì¹˜ê°€ 10% ì´ìƒ
        return RobustScaler(), f"RobustScaler (outlier_ratio: {outlier_ratio:.3f})"
    elif skewness > 2:  # ë§¤ìš° ì¹˜ìš°ì¹œ ë¶„í¬
        return PowerTransformer(method='yeo-johnson'), f"PowerTransformer (skewness: {skewness:.3f})"
    elif data_range < 100 and clean_data.min() >= 0:  # ì‘ì€ ë²”ìœ„ì˜ ì–‘ìˆ˜ ë°ì´í„°
        return MinMaxScaler(), f"MinMaxScaler (range: {data_range:.1f})"
    else:  # ì¼ë°˜ì ì¸ ê²½ìš°
        return StandardScaler(), f"StandardScaler (default)"

def normalize_features(df, feature_types):
    """í”¼ì²˜ë“¤ì„ ì •ê·œí™”í•©ë‹ˆë‹¤."""
    
    df_normalized = df.copy()
    scalers = {}
    scaling_info = {}
    
    # ì •ê·œí™”í•  í”¼ì²˜ë“¤ (numerical í”¼ì²˜ë§Œ)
    features_to_scale = feature_types['numerical']
    
    print(f"ì •ê·œí™” ëŒ€ìƒ í”¼ì²˜ ìˆ˜: {len(features_to_scale)}")
    
    for feature in features_to_scale:
        print(f"\nì²˜ë¦¬ ì¤‘: {feature}")
        
        # ê²°ì¸¡ê°’ ì„ì‹œ ì²˜ë¦¬ (ì¤‘ìœ„ìˆ˜ë¡œ ì±„ì›€)
        original_na_mask = df_normalized[feature].isna()
        if original_na_mask.any():
            median_val = df_normalized[feature].median()
            df_normalized[feature].fillna(median_val, inplace=True)
            print(f"  - ê²°ì¸¡ê°’ {original_na_mask.sum()}ê°œë¥¼ ì¤‘ìœ„ìˆ˜ {median_val:.2f}ë¡œ ì„ì‹œ ì²˜ë¦¬")
        
        # ìµœì  ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ
        scaler, reason = get_optimal_scaler(df_normalized[feature], feature)
        
        if scaler is None:
            print(f"  - ìŠ¤í‚µ (ìœ íš¨ ë°ì´í„° ì—†ìŒ)")
            continue
        
        # ìŠ¤ì¼€ì¼ë§ ì ìš©
        try:
            # 2D ë°°ì—´ë¡œ ë³€í™˜
            feature_values = df_normalized[feature].values.reshape(-1, 1)
            scaled_values = scaler.fit_transform(feature_values)
            df_normalized[feature] = scaled_values.flatten()
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë° ì •ë³´ ì €ì¥
            scalers[feature] = scaler
            scaling_info[feature] = {
                'scaler_type': type(scaler).__name__,
                'reason': reason,
                'original_mean': float(df[feature].mean()) if not df[feature].isna().all() else None,
                'original_std': float(df[feature].std()) if not df[feature].isna().all() else None,
                'scaled_mean': float(df_normalized[feature].mean()),
                'scaled_std': float(df_normalized[feature].std())
            }
            
            print(f"  - ì ìš©: {reason}")
            print(f"  - ë³€í™˜ ì „: í‰ê·  {df[feature].mean():.3f}, í‘œì¤€í¸ì°¨ {df[feature].std():.3f}")
            print(f"  - ë³€í™˜ í›„: í‰ê·  {df_normalized[feature].mean():.3f}, í‘œì¤€í¸ì°¨ {df_normalized[feature].std():.3f}")
            
        except Exception as e:
            print(f"  - ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
        
        # ì›ë³¸ ê²°ì¸¡ê°’ íŒ¨í„´ ë³µì›
        if original_na_mask.any():
            df_normalized.loc[original_na_mask, feature] = np.nan
            print(f"  - ì›ë³¸ ê²°ì¸¡ê°’ íŒ¨í„´ ë³µì› ì™„ë£Œ")
    
    return df_normalized, scalers, scaling_info

def validate_scaling_quality(df_original, df_scaled, feature_types):
    """ìŠ¤ì¼€ì¼ë§ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    
    validation_results = {}
    
    numerical_features = feature_types['numerical']
    
    for feature in numerical_features:
        if feature not in df_scaled.columns:
            continue
        
        # ê²°ì¸¡ê°’ ì œì™¸í•˜ê³  ë¹„êµ
        original_clean = df_original[feature].dropna()
        scaled_clean = df_scaled[feature].dropna()
        
        if len(original_clean) == 0 or len(scaled_clean) == 0:
            continue
        
        # ê²€ì¦ ê¸°ì¤€ë“¤
        criteria = {}
        
        # 1. ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ì˜ í‰ê· ì´ 0 ê·¼ì²˜ì¸ì§€ (StandardScalerì˜ ê²½ìš°)
        criteria['mean_near_zero'] = abs(scaled_clean.mean()) < 0.1
        
        # 2. ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ì˜ í‘œì¤€í¸ì°¨ê°€ 1 ê·¼ì²˜ì¸ì§€ (StandardScalerì˜ ê²½ìš°)
        criteria['std_near_one'] = 0.8 < scaled_clean.std() < 1.2
        
        # 3. ê²°ì¸¡ê°’ íŒ¨í„´ì´ ë³´ì¡´ë˜ì—ˆëŠ”ì§€
        original_na_pattern = df_original[feature].isna()
        scaled_na_pattern = df_scaled[feature].isna()
        criteria['na_pattern_preserved'] = original_na_pattern.equals(scaled_na_pattern)
        
        # 4. ìƒëŒ€ì  ìˆœì„œê°€ ë³´ì¡´ë˜ì—ˆëŠ”ì§€ (ìƒê´€ê´€ê³„ í™•ì¸)
        if len(original_clean) > 1 and len(scaled_clean) > 1:
            correlation = original_clean.corr(scaled_clean)
            criteria['order_preserved'] = correlation > 0.95
        else:
            criteria['order_preserved'] = True
        
        # 5. ì´ìƒì¹˜ê°€ ê³¼ë„í•˜ê²Œ ì¦ê°€í•˜ì§€ ì•Šì•˜ëŠ”ì§€
        original_outliers = ((original_clean - original_clean.mean()).abs() > 3 * original_clean.std()).sum()
        scaled_outliers = ((scaled_clean - scaled_clean.mean()).abs() > 3 * scaled_clean.std()).sum()
        criteria['outliers_controlled'] = scaled_outliers <= original_outliers * 1.5
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        quality_score = sum(criteria.values()) / len(criteria)
        
        validation_results[feature] = {
            'criteria': criteria,
            'quality_score': quality_score,
            'passed': quality_score >= 0.8  # 80% ì´ìƒ ê¸°ì¤€ í†µê³¼
        }
    
    return validation_results

def create_scaling_visualizations(df_original, df_scaled, feature_types, output_dir):
    """ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    
    numerical_features = feature_types['numerical'][:9]  # ì²˜ìŒ 9ê°œë§Œ ì‹œê°í™”
    
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))
    axes = axes.flatten()
    
    for i, feature in enumerate(numerical_features):
        if i >= 9:
            break
        
        ax = axes[i]
        
        # ê²°ì¸¡ê°’ ì œì™¸
        original_clean = df_original[feature].dropna()
        scaled_clean = df_scaled[feature].dropna()
        
        if len(original_clean) == 0:
            ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes)
            ax.set_title(feature)
            continue
        
        # ë¶„í¬ ë¹„êµ
        ax.hist(original_clean, bins=30, alpha=0.5, label='Original', density=True)
        ax.hist(scaled_clean, bins=30, alpha=0.5, label='Scaled', density=True)
        
        ax.set_title(f'{feature}\nOrig: Î¼={original_clean.mean():.2f}, Ïƒ={original_clean.std():.2f}\n'
                    f'Scaled: Î¼={scaled_clean.mean():.2f}, Ïƒ={scaled_clean.std():.2f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    # ë‚¨ì€ subplot ì œê±°
    for i in range(len(numerical_features), 9):
        fig.delaxes(axes[i])
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'scaling_effects_comparison.png'), 
                dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()

def generate_comprehensive_report(df_original, df_scaled, feature_types, scaling_info, 
                                validation_results, output_dir):
    """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    report = []
    report.append("# Task 3.4: Feature Normalization and Scaling ê²°ê³¼ ë³´ê³ ì„œ\n")
    report.append(f"ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # ì „ì²´ ìš”ì•½
    total_features = len(df_original.columns)
    numerical_features = len(feature_types['numerical'])
    scaled_features = len([f for f in feature_types['numerical'] if f in scaling_info])
    
    report.append("## ğŸ“Š ì „ì²´ ìš”ì•½\n")
    report.append(f"- **ì „ì²´ í”¼ì²˜ ìˆ˜**: {total_features}ê°œ")
    report.append(f"- **ìˆ˜ì¹˜í˜• í”¼ì²˜ ìˆ˜**: {numerical_features}ê°œ")
    report.append(f"- **ìŠ¤ì¼€ì¼ë§ ì ìš©**: {scaled_features}ê°œ ({scaled_features/numerical_features*100:.1f}%)")
    report.append(f"- **ë°ì´í„° í–‰ ìˆ˜**: {len(df_scaled):,}í–‰\n")
    
    # í”¼ì²˜ íƒ€ì…ë³„ ë¶„ë¥˜
    report.append("## ğŸ·ï¸ í”¼ì²˜ íƒ€ì…ë³„ ë¶„ë¥˜\n")
    for type_name, features in feature_types.items():
        if features:
            report.append(f"### {type_name.title()} í”¼ì²˜ ({len(features)}ê°œ)")
            for feature in features:
                report.append(f"- `{feature}`")
            report.append("")
    
    # ìŠ¤ì¼€ì¼ë§ ê²°ê³¼
    report.append("## âš™ï¸ ìŠ¤ì¼€ì¼ë§ ì ìš© ê²°ê³¼\n")
    for feature, info in scaling_info.items():
        report.append(f"### {feature}")
        report.append(f"- **ìŠ¤ì¼€ì¼ëŸ¬**: {info['scaler_type']}")
        report.append(f"- **ì„ íƒ ì´ìœ **: {info['reason']}")
        if info['original_mean'] is not None:
            report.append(f"- **ë³€í™˜ ì „**: í‰ê·  {info['original_mean']:.3f}, í‘œì¤€í¸ì°¨ {info['original_std']:.3f}")
        report.append(f"- **ë³€í™˜ í›„**: í‰ê·  {info['scaled_mean']:.3f}, í‘œì¤€í¸ì°¨ {info['scaled_std']:.3f}")
        report.append("")
    
    # í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
    report.append("## âœ… í’ˆì§ˆ ê²€ì¦ ê²°ê³¼\n")
    passed_count = sum(1 for result in validation_results.values() if result['passed'])
    total_validated = len(validation_results)
    
    report.append(f"**ê²€ì¦ í†µê³¼ìœ¨**: {passed_count}/{total_validated} ({passed_count/total_validated*100:.1f}%)\n")
    
    for feature, result in validation_results.items():
        status = "âœ… í†µê³¼" if result['passed'] else "âŒ ì‹¤íŒ¨"
        report.append(f"### {feature} - {status} (ì ìˆ˜: {result['quality_score']:.2f})")
        
        for criterion, passed in result['criteria'].items():
            symbol = "âœ“" if passed else "âœ—"
            report.append(f"- {symbol} {criterion}")
        report.append("")
    
    # ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´ í™•ì¸
    report.append("## ğŸ• ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´ í™•ì¸\n")
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ë¹„êµ
    target_corr_original = {}
    target_corr_scaled = {}
    
    for feature in feature_types['numerical']:
        if feature in df_original.columns and feature in df_scaled.columns:
            orig_corr = df_original['power_consumption'].corr(df_original[feature])
            scaled_corr = df_scaled['power_consumption'].corr(df_scaled[feature])
            
            if not pd.isna(orig_corr) and not pd.isna(scaled_corr):
                target_corr_original[feature] = orig_corr
                target_corr_scaled[feature] = scaled_corr
    
    report.append("### íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ë³´ì¡´ë„")
    report.append("| í”¼ì²˜ëª… | ì›ë³¸ ìƒê´€ê´€ê³„ | ìŠ¤ì¼€ì¼ë§ í›„ | ì°¨ì´ | ë³´ì¡´ë„ |")
    report.append("|--------|-------------|------------|------|--------|")
    
    correlation_changes = []
    for feature in target_corr_original:
        orig = target_corr_original[feature]
        scaled = target_corr_scaled[feature]
        diff = abs(orig - scaled)
        preservation = "ìš°ìˆ˜" if diff < 0.01 else "ì–‘í˜¸" if diff < 0.05 else "ì£¼ì˜"
        
        report.append(f"| {feature} | {orig:.3f} | {scaled:.3f} | {diff:.3f} | {preservation} |")
        correlation_changes.append(diff)
    
    if correlation_changes:
        avg_change = np.mean(correlation_changes)
        report.append(f"\n**í‰ê·  ìƒê´€ê´€ê³„ ë³€í™”**: {avg_change:.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    
    # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    report.append("\n## ğŸš€ ë‹¤ìŒ ë‹¨ê³„: Task 3.5\n")
    report.append("**Time-Aware Data Splitting**ì„ ìœ„í•œ ì™„ì „í•œ ì •ê·œí™” ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    report.append("\n### í™œìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤")
    report.append("- `electricity_data_normalized.csv`: ì •ê·œí™”ëœ ì „ì²´ ë°ì´í„°ì…‹")
    report.append("- `scalers.joblib`: ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ë“¤ (ë¯¸ë˜ ë°ì´í„° ì ìš©ìš©)")
    report.append("- `scaling_metadata.json`: ìƒì„¸ ìŠ¤ì¼€ì¼ë§ ì •ë³´")
    
    # ë³´ê³ ì„œ ì €ì¥
    with open(os.path.join(output_dir, 'feature_normalization_report.txt'), 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    return report

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì…ë ¥ ë°ì´í„° ë¡œë“œ
    input_path = 'results/preprocessing/03_derived_variables/electricity_data_with_core_derived.csv'
    df = pd.read_csv(input_path)
    
    print("=" * 80)
    print("Task 3.4: Feature Normalization and Scaling")
    print("=" * 80)
    print(f"ì…ë ¥ ë°ì´í„°: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
    print(f"ë‚ ì§œ ë²”ìœ„: {df['date'].min()} ~ {df['date'].max()}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = 'results/preprocessing/04_normalization'
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. í”¼ì²˜ íƒ€ì… ë¶„ì„
    print("\n1ë‹¨ê³„: í”¼ì²˜ íƒ€ì… ë¶„ì„")
    feature_types = analyze_feature_types(df)
    
    for type_name, features in feature_types.items():
        if features:
            print(f"  {type_name}: {len(features)}ê°œ")
    
    # 2. í”¼ì²˜ ì •ê·œí™”
    print("\n2ë‹¨ê³„: í”¼ì²˜ ì •ê·œí™” ìˆ˜í–‰")
    df_normalized, scalers, scaling_info = normalize_features(df, feature_types)
    
    # 3. í’ˆì§ˆ ê²€ì¦
    print("\n3ë‹¨ê³„: ìŠ¤ì¼€ì¼ë§ í’ˆì§ˆ ê²€ì¦")
    validation_results = validate_scaling_quality(df, df_normalized, feature_types)
    
    passed_count = sum(1 for result in validation_results.values() if result['passed'])
    total_count = len(validation_results)
    print(f"í’ˆì§ˆ ê²€ì¦ í†µê³¼: {passed_count}/{total_count} ({passed_count/total_count*100:.1f}%)")
    
    # 4. ì‹œê°í™”
    print("\n4ë‹¨ê³„: ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ ì‹œê°í™”")
    create_scaling_visualizations(df, df_normalized, feature_types, output_dir)
    
    # 5. ê²°ê³¼ ì €ì¥
    print("\n5ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
    
    # ì •ê·œí™”ëœ ë°ì´í„°ì…‹ ì €ì¥
    normalized_output_path = os.path.join(output_dir, 'electricity_data_normalized.csv')
    df_normalized.to_csv(normalized_output_path, index=False, encoding='utf-8')
    print(f"ì •ê·œí™” ë°ì´í„°ì…‹ ì €ì¥: {normalized_output_path}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ë“¤ ì €ì¥
    scalers_path = os.path.join(output_dir, 'scalers.joblib')
    joblib.dump(scalers, scalers_path)
    print(f"ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ë“¤ ì €ì¥: {scalers_path}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥ (JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜)
    metadata = {
        'feature_types': feature_types,
        'scaling_info': scaling_info,
        'validation_results': {k: {
            'quality_score': float(v['quality_score']),
            'passed': bool(v['passed']),
            'criteria': {kk: bool(vv) for kk, vv in v['criteria'].items()}
        } for k, v in validation_results.items()},
        'processing_datetime': datetime.now().isoformat(),
        'input_shape': list(df.shape),
        'output_shape': list(df_normalized.shape)
    }
    
    metadata_path = os.path.join(output_dir, 'scaling_metadata.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    # 6. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
    print("\n6ë‹¨ê³„: ì¢…í•© ë³´ê³ ì„œ ìƒì„±")
    report = generate_comprehensive_report(df, df_normalized, feature_types, scaling_info, 
                                         validation_results, output_dir)
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 80)
    print("ğŸ‰ Task 3.4 ì™„ë£Œ!")
    print("=" * 80)
    print(f"âœ… ì •ê·œí™”ëœ ë°ì´í„°ì…‹: {df_normalized.shape[0]:,}í–‰ Ã— {df_normalized.shape[1]}ì—´")
    print(f"âœ… ìŠ¤ì¼€ì¼ë§ ì ìš© í”¼ì²˜: {len(scaling_info)}ê°œ")
    print(f"âœ… í’ˆì§ˆ ê²€ì¦ í†µê³¼: {passed_count}/{total_count}ê°œ")
    print(f"âœ… ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    
    return df_normalized, scalers, validation_results

if __name__ == "__main__":
    main() 